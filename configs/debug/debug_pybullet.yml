# This is the main config file for default parameters for all hyperparameter and task parameter.
# Every config file should import this. The project will run with these parameters, no matter what.

name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "gpu"
job-name: "lts_gns"    # this will be the experiments name in slurm

# Required - Cluster Specific
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 2
mem-per-cpu: 15000
time: 1440  # in minutes

sbatch_args: # Dictionary of SBATCH keywords and arguments
  gres: "gpu:1"

slurm_log: "./output/slurmlog" # optional. dir in which slurm output and error logs will be saved.
---
name: "p0104_local_run"   # MUST BE DEFAULT

import_path: ../default.yml
import_exp: DEFAULT

# Implementation default parameters
path: "./output/cw2_data"   # location to save reports in
repetitions: 1 # number of times one set of parameters is run
reps_per_job: 1 # number of repetitions in each job. useful for paralellization. defaults to 1.
reps_in_parallel: 1 # number of repetitions in each job that are executed in parallel. defaults to 1.

iterations: 2000  # number of iterations of the algorithm

params:
  device: "cuda"

  random_seeds:
    numpy: 69  # can be null for no random seed, "default" for numpy_seed==repetition, or an integer value
    pytorch: 420  # can be null for no random seed, "tied" for pytorch_seed==numpy_seed, or an integer value
    # nice!

  recording:
    config: True  # whether to save the config of each trial as a .yaml file or not
    defaults: True  # whether to log default values (time and memory usage)
    scalars: True   # whether to log scalar values over time, also on hard disk.
    visualizations: True  # whether to log an animation
    checkpoint: True  # whether to log checkpoints of the task. This can e.g, be the networks used for the individual
    # algorithms
    checkpoint_frequency: 100  # checkpointing can quickly become expensive for larger models. We also may only want
    # to do this every n iterations.
    wandb:
      enabled: True  # whether to use the wandb logger or not
      project_name: LTS_GNS  # name of the project
      entity: null  # name of the entity to log to. Will default to your private wandb account
      job_type: null

      # only used categorizing wandb projects
      tags: null  # list of custom tags to sort/find these runs by
      start_method: "thread"  # start method for wandb. "thread" is recommended for slurm and on your cluster.
      # null will use the default wandb start method,
      # which is "fork" on linux and "spawn" on windows (according to copilot)

  algorithm:
    name: mgn

    common:
      # parameters that are used by all algorithms
      training:
        batches_per_epoch: 5   # how may minibatches per recorded step. After every epoch, there is the small evaluation of the val tasks.
        add_training_noise: True
      evaluation:
        large:
          recompute_edges: True  # whether to recompute the (mesh-collider) edges for the evaluation or not.
          # if both true and false are in the list, both options are evaluated.
          frequency: 100   # after how many small evals there is a large one
          multi_step_evaluations: []
          animation_task_indices: [ 0, 1, 2]   # which tasks to plot.
          initial_eval: True  # whether to do an initial evaluation after first iteration or not (is slower)
      simulator:
        gnn:
          latent_dimension: 128
          share_base: False
          base:
            assert_graph_shapes: False
            scatter_reducers: mean
            # Also supports std, but that is unstable
            create_graph_copy: True  # whether to create a copy of the used graph before the forward pass or not
            stack:
              layer_norm: inner # which kind of layer normalization to use. null/None for no layer norm,
              # "outer" for layer norm around each full message passing step, "inner" for layer norm after each message
              num_blocks: 5
              residual_connections: inner
              outer_aggregation: concatenation
              mlp:
                activation_function: leakyrelu
                num_layers: 1
                add_output_layer: False
                regularization:
                  dropout: 0
                  spectral_norm: False
                  latent_normalization: null
            embedding:
              mlp:
                activation_function: leakyrelu
                num_layers: 1
                add_output_layer: False
                regularization:
                  dropout: 0
                  spectral_norm: False
                  latent_normalization: null
        decoder:
          latent_dimension: 64
        optimizer:
          gnn_learning_rate: 5.0e-4
          decoder_learning_rate: 5.0e-4

    lts_gns:
      training:
        num_z_samples_for_elbo_estimate: 32  # how many samples to use for the elbo estimate for the GNN training part
        num_posterior_learner_steps: 1
      evaluation:
        small:
          num_posterior_learner_step: 10
        large:
          num_posterior_learner_steps: 200
          training_gmm_indices: [ ]  # which gmms to use for the training set

      simulator:
        use_posterior_target_network: False  # whether to use a target network for the posterior learner or not
        posterior_target_network_rate: 0.99  # 1-rate at which the target network is updated
        decoder:
          z_embedding: null  # whether to embed the z into the hidden dimension of the decoder or not.
        prior:
          n_components: 4
          prior_scale: 1.0
          initial_var: 1.0
        likelihood:
          std: 0.01
          graph_aggregation: mean
          timestep_aggregation: sum

      posterior_learner:
        name: multi_daft_posterior_learner
        constant_posterior_learner:
          d_z: 1
        task_properties_learner: # it outputs the task properties as the latent z var
          dummy: null   # no arguments for this learner, but it is required to not have an empty dict
        multi_daft_posterior_learner:
          prior_scale: 1.0    # Standard deviation for distribution which samples the means parameter
          initial_var: 1.0    # Initial Covars is initialized as torch.eye(d_z) * initial_var
          n_components: 3
          d_z: 2
          logging_frequency: 10   # If logging is enabled in the fit() method, the elbo and other metrics are logged every
          # logging_frequency steps. The total logs are bundled into a wandb.Table and plotted
          n_samples_per_comp: 20
          n_samples_per_comp_for_elbo_logging: 100  # elbo between GMM and target density is estimated by sampling from the GMM.
          # This parameter determines how many samples are used for this estimation.
          mini_batch_size_for_target_density: 100  # if n_samples_per_comp (or n_samples_per_comp_for_elbo_logging) * n_components > this, the call for the target_dist
          # log_density() call is split into batches of this size. (For memory reasons)
          more:
            component_kl_bound: 0.01
            global_upper_bound: 1.0e+10
            max_prec_element_value: 1.0e+8  # maximum value of precision matrix elements, updates with higher values are rejected
            max_dual_steps: 100
            global_lower_bound: 0.0
            dual_conv_tol: 0.1
            use_warm_starts: False
            warm_start_interval_size: 100
    mgn:
      dummy: null

  env:
    name: pybullet_square_cloth
    common:  # common parameters that are used by all environments/tasks
      visualization:
        num_frames: -1
      preprocess:
        save_load:
          path_to_datasets: ../datasets/lts_gns/
          load_preprocessed_graph: False  # no load/save shenanigans
          save_preprocessed_graph: False
        task_properties_input_selection: null
      postprocess:
        auxiliary_tasks:
          auxiliary_tasks_per_task: 8
          min_task_length: 1
          max_task_length: 50
          sampling_type: slice
        context_eval_tasks:
          min_context_task_length: 1
          max_context_task_length: 25
          sampling_type: slice
        batch:
          max_train_batch_size: 50
        input_mesh_noise: 0.01
        input_point_cloud_noise: 0.0
        euclidian_distance_feature: True
      debug:
        max_tasks_per_split: 5

