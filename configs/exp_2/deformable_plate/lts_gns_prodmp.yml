# This is the main config file for default parameters for all hyperparameter and task parameter.
# Every config file should import this. The project will run with these parameters, no matter what.

name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "gpu"
job-name: "lts_gns_prodmp"    # this will be the experiments name in slurm

# Required - Cluster Specific
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 2
mem-per-cpu: 15000
time: 2880  # in minutes

sbatch_args: # Dictionary of SBATCH keywords and arguments
  gres: "gpu:1"


slurm_log: "./output/slurmlog" # optional. dir in which slurm output and error logs will be saved.
---
name: "e2_lts_gns_prodmp_deformable_plate"

import_path: ../../default.yml
import_exp: DEFAULT

# Implementation default parameters
path: "./output/cw2_data"   # location to save reports in
repetitions: 5 # number of times one set of parameters is run
reps_per_job: 1 # number of repetitions in each job. useful for paralellization. defaults to 1.
reps_in_parallel: 1 # number of repetitions in each job that are executed in parallel. defaults to 1.

iterations: 1000  # number of iterations of the algorithm

params:
  exp_version: 2
  recording:
    checkpoint_frequency: 100  # checkpointing can quickly become expensive for larger models. We also may only want
    # to do this every n iterations.
    wandb:
      enabled: True  # whether to use the wandb logger or not
      entity: kit-alr

  algorithm:
    name: lts_gns_prodmp
    common:
      # parameters that are used by all algorithms
      training:
        batches_per_epoch: 500   # how may minibatches per recorded step. After every epoch, there is the small evaluation of the val tasks.
        add_training_noise: False
      evaluation:
        large:
          recompute_edges: True  # whether to recompute the (mesh-collider) edges for the evaluation or not.
          # if both true and false are in the list, both options are evaluated.
          frequency: 100   # after how many small evals there is a large one
          multi_step_evaluations: [ 1, 5, 10, 49 ]
          animation_task_indices: [0, 1, 10]   # which tasks to plot.
          initial_eval: True  # whether to do an initial evaluation after first iteration or not (is slower)
      simulator:
        decoder:
          latent_dimension: 256
        optimizer:
          gnn_learning_rate: 5.0e-4
          decoder_learning_rate: 5.0e-4

    lts_gns_prodmp:
      evaluation:
        small:
          num_posterior_learner_step: 10
        large:
          num_posterior_learner_steps: 100
          num_z_samples_per_animation_task: 1 # how many different animations with different z samples should be computed
          training_gmm_indices: []  # which gmms to use for the training set
      simulator:
        decoder:
          z_embedding: null  # whether to embed the z into the hidden dimension of the decoder or not.
          feature_dropout: null  # whether to use feature dropout or not. If null, there is no dropout. If a float, this
          # is the dropout rate.
          velocity_decoder: False  # whether to use a velocity decoder or not. If true, the decoder outputs the velocity as well
        prior:
          n_components: 4
          prior_scale: 1.0
          initial_var: 1.0
        likelihood:
          std: 0.01
          chamfer_std: 0.01
        mp_config:
          min_tau: 0.3
          max_tau: 3.0
          tau: 1.0
          learn_tau: True
          mp_type: prodmp
          mp_args:
            num_basis: 10
            relative_goal: True
            alpha: 25.0
            alpha_phase: 2.0
            basis_bandwidth_factor: 1.0
            num_basis_outside: 0
            auto_scale_basis: True
            weights_scale: 1.0

      posterior_learner:
        name: multi_daft_posterior_learner
        multi_daft_posterior_learner:
          prior_scale: 1.0    # Standard deviation for distribution which samples the means parameter
          initial_var: 1.0    # Initial Covars is initialized as torch.eye(d_z) * initial_var
          n_components: 3
          d_z: 8
          logging_frequency: 10   # If logging is enabled in the fit() method, the elbo and other metrics are logged every
          # logging_frequency steps. The total logs are bundled into a wandb.Table and plotted
          n_samples_per_comp: 40
          n_samples_per_comp_for_elbo_logging: 80  # elbo between GMM and target density is estimated by sampling from the GMM.
          # This parameter determines how many samples are used for this estimation.
          mini_batch_size_for_target_density: 120  # if n_samples_per_comp (or n_samples_per_comp_for_elbo_logging) * n_components > this, the call for the target_dist
          # log_density() call is split into batches of this size. (For memory reasons)
          sample_at_mean: False

  env:
    name: deformable_plate
    common:  # common parameters that are used by all environments/tasks
      statistics:
        use_output_normalizer: False  # if true, will use the tasks statistics  to normalize the GNS output for training
      preprocess:
        save_load:
          path_to_datasets: ../datasets/lts_gns/
          load_preprocessed_graph: False  # no load/save shenanigans
          save_preprocessed_graph: False
        task_properties_input_selection: null
        use_collider_velocities: True
      postprocess:
        auxiliary_tasks:
          auxiliary_tasks_per_task: 30
          min_task_length: 1
          max_task_length: 30
          sampling_type: uniform
        context_eval_tasks:
          auxiliary_tasks_per_task: 1
          min_task_length: 1
          max_task_length: 10
          sampling_type: uniform
        euclidian_distance_feature: True
      debug:
        max_tasks_per_split: null
    deformable_plate:
      preprocess:
        use_point_cloud: False
      postprocess:
        batch:
          use_adaptive_batch_size: True
          adaptive_batch_size:
            factor_per_context_point: 0.2
            batch_cost: 60
          max_eval_batch_size: 1



