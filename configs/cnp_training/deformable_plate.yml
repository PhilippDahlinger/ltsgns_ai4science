# This is the main config file for default parameters for all hyperparameter and task parameter.
# Every config file should import this. The project will run with these parameters, no matter what.

name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "gpu"
job-name: "lts_gns"    # this will be the experiments name in slurm

# Required - Cluster Specific
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 2
mem-per-cpu: 15000
time: 2880  # in minutes

sbatch_args: # Dictionary of SBATCH keywords and arguments
  gres: "gpu:1"
  exclude: "node1,node2"

slurm_log: "./output/slurmlog" # optional. dir in which slurm output and error logs will be saved.
---
name: p4002_cnp_deformable_plate

import_path: ../default.yml
import_exp: DEFAULT

repetitions: 50 # number of times one set of parameters is run
reps_per_job: 1 # number of repetitions in each job. useful for paralellization. defaults to 1.
reps_in_parallel: 1 # number of repetitions in each job that are executed in parallel. defaults to 1.

iterations: 1000 # number of iterations of the algorithm, usually about 500 when it starts overfitting

params:
  recording:
    wandb:
      enabled: True  # right now disabled to not spam runs during debugging

  device: cuda

  algorithm:
    name: cnp
    common:
      training:
        batches_per_epoch: 500
        # how may minibatches per recorded step. After every epoch, there is the small evaluation of the val tasks.
      evaluation:
        large:
          multi_step_evaluations: [1, 5, 10 ]
          animation_task_indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 27, 28, 29]   # which tasks to plot.
          recompute_edges: True
          initial_eval: True
      simulator:
        loss: mse
#        chamfer:
#          density_aware: True

    cnp:
      encoder:
        d_r: 8
        gnn:
          latent_dimension: 64
          share_base: False
          base:
            assert_graph_shapes: False
            scatter_reduce: mean  # how to reduce the edge features for node updates, and the edge and node features for
            # global updates
            # Can be either a single operation or any list of the following: "mean", "sum", "min" or "max", "std".
            create_graph_copy: True  # whether to create a copy of the used graph before the forward pass or not
            stack:
              layer_norm: inner # which kind of layer normalization to use. null/None for no layer norm,
              # "outer" for layer norm around each full message passing step, "inner" for layer norm after each message
              num_steps: 5
              residual_connections: inner
              mlp:
                activation_function: leakyrelu
                num_layers: 1
                add_output_layer: False
                regularization:
                  dropout: 0
                  spectral_norm: False
                  latent_normalization: null
            embedding:
              mlp:
                activation_function: leakyrelu
                num_layers: 1
                add_output_layer: False
                regularization:
                  dropout: 0
                  spectral_norm: False
                  latent_normalization: null

  env:
    name: deformable_plate
    common:
      preprocess:
        train_label_noise: 0.0
        include_pc2mesh: False
      postprocess:
        auxiliary_tasks:
          auxiliary_tasks_per_task: 8
      debug:
        max_tasks_per_split: null


