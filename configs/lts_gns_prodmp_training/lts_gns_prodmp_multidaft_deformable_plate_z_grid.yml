# This is the main config file for default parameters for all hyperparameter and task parameter.
# Every config file should import this. The project will run with these parameters, no matter what.

name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "gpu"
job-name: "lts_gns_prodmp"    # this will be the experiments name in slurm

# Required - Cluster Specific
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 2
mem-per-cpu: 15000
time: 2880  # in minutes

sbatch_args: # Dictionary of SBATCH keywords and arguments
  gres: "gpu:1"


slurm_log: "./output/slurmlog" # optional. dir in which slurm output and error logs will be saved.
---
name: p5217_lts_gns_prodmp_z_grid

import_path: ../default.yml
import_exp: DEFAULT

repetitions: 1 # number of times one set of parameters is run
reps_per_job: 1 # number of repetitions in each job. useful for paralellization. defaults to 1.
reps_in_parallel: 1 # number of repetitions in each job that are executed in parallel. defaults to 1.

iterations: 1000 # number of iterations of the algorithm, usually about 500 when it starts overfitting

params:
  recording:
    wandb:
      enabled: True  # right now disabled to not spam runs during debugging

  device: cuda

  algorithm:
    name: lts_gns_prodmp
    common:
      training:
        batches_per_epoch: 500
        # how may minibatches per recorded step. After every epoch, there is the small evaluation of the val tasks.
      evaluation:
        large:
          frequency: 100
          multi_step_evaluations: [1, 5, 10]
          animation_task_indices: [0, 1, 9, 10]   # which tasks to plot.
          recompute_edges: True
          initial_eval: True
      simulator:
        loss: mse
        chamfer:
          density_aware: True

    lts_gns_prodmp:
      posterior_learner:
        name: multi_daft_posterior_learner
        multi_daft_posterior_learner:
          prior_scale: 1.0    # Standard deviation for distribution which samples the means parameter
          initial_var: 1.0    # Initial Covars is initialized as torch.eye(d_z) * initial_var
          n_components: 1
          d_z: 2
          logging_frequency: 10   # If logging is enabled in the fit() method, the elbo and other metrics are logged every
          # logging_frequency steps. The total logs are bundled into a wandb.Table and plotted
          n_samples_per_comp: 20
      evaluation:
        small:
          num_posterior_learner_step: 10
        large:
          num_posterior_learner_steps: 100
          training_gmm_indices: [ ]
          num_z_samples_per_animation_task: 1
      simulator:
        decoder:
          velocity_decoder: False
        likelihood:
          std: 0.01
          chamfer_std: 0.1


  env:
    name: deformable_plate
    common:
      preprocess:
        train_label_noise: 0.0
        include_pc2mesh: False
        use_collider_velocities: True
      debug:
        max_tasks_per_split: null
    deformable_plate:
      preprocess:
        use_point_cloud: False
      postprocess:
        batch:
          use_adaptive_batch_size: True
          max_train_batch_size: 10
          max_eval_batch_size: 10

list:
  index: [1, 2, 3, 4]
  algorithm:
    lts_gns_prodmp:
      posterior_learner:
        multi_daft_posterior_learner:
          d_z: [2, 4, 8, 16]
          n_samples_per_comp: [20, 30, 40, 60]

