# This is the main config file for default parameters for all hyperparameter and task parameter.
# Every config file should import this. The project will run with these parameters, no matter what.

name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "gpu"
job-name: "lts_gns"    # this will be the experiments name in slurm

# Required - Cluster Specific
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 2
mem-per-cpu: 15000
time: 1440  # in minutes

sbatch_args: # Dictionary of SBATCH keywords and arguments
  gres: "gpu:1"

slurm_log: "./output/slurmlog" # optional. dir in which slurm output and error logs will be saved.
---
name: "p0301_mgn"   # MUST BE DEFAULT

import_path: ../default.yml
import_exp: DEFAULT

# Implementation default parameters
path: "./output/cw2_data"   # location to save reports in
repetitions: 2 # number of times one set of parameters is run
reps_per_job: 1 # number of repetitions in each job. useful for paralellization. defaults to 1.
reps_in_parallel: 1 # number of repetitions in each job that are executed in parallel. defaults to 1.

iterations: 2000  # number of iterations of the algorithm

params:
  device: "cuda"

  random_seeds:
    numpy: 69  # can be null for no random seed, "default" for numpy_seed==repetition, or an integer value
    pytorch: 420  # can be null for no random seed, "tied" for pytorch_seed==numpy_seed, or an integer value
    # nice!

  recording:
    config: True  # whether to save the config of each trial as a .yaml file or not
    defaults: True  # whether to log default values (time and memory usage)
    scalars: True   # whether to log scalar values over time, also on hard disk.
    visualizations: True  # whether to log an animation
    checkpoint: True  # whether to log checkpoints of the task. This can e.g, be the networks used for the individual
    # algorithms
    checkpoint_frequency: 100  # checkpointing can quickly become expensive for larger models. We also may only want
    # to do this every n iterations.
    wandb:
      enabled: True  # whether to use the wandb logger or not
      project_name: LTS_GNS  # name of the project
      entity: null  # name of the entity to log to. Will default to your private wandb account
      job_type: null

      # only used categorizing wandb projects
      tags: null  # list of custom tags to sort/find these runs by
      start_method: "thread"  # start method for wandb. "thread" is recommended for slurm and on your cluster.
      # null will use the default wandb start method,
      # which is "fork" on linux and "spawn" on windows (according to copilot)

  algorithm:
    name: mgn

    common:
      # parameters that are used by all algorithms
      training:
        batches_per_epoch: 500   # how may minibatches per recorded step. After every epoch, there is the small evaluation of the val tasks.
        add_training_noise: True
      evaluation:
        large:
          recompute_edges: True  # whether to recompute the (mesh-collider) edges for the evaluation or not.
          # if both true and false are in the list, both options are evaluated.
          frequency: 100   # after how many small evals there is a large one
          multi_step_evaluations: [1, 5, 10]
          animation_task_indices: [ 0, 1, 2, 3, 4, 5, 6, 7 ]   # which tasks to plot.
          initial_eval: True  # whether to do an initial evaluation after first iteration or not (is slower)
      simulator:
        decoder:
          latent_dimension: 64
        optimizer:
          gnn_learning_rate: 5.0e-4
          decoder_learning_rate: 5.0e-4
    mgn:
      dummy: null

  env:
    name: pybullet_uniform
    common: # common parameters that are used by all environments/tasks
      preprocess:
        save_load:
          path_to_datasets: ../datasets/lts_gns/
          load_preprocessed_graph: False  # no load/save shenanigans
          save_preprocessed_graph: False
        task_properties_input_selection: null
      postprocess:
        auxiliary_tasks:
          auxiliary_tasks_per_task: 16
          min_task_length: 1
          max_task_length: 50
          sampling_type: slice
        context_eval_tasks:
          min_context_task_length: 25
          max_context_task_length: 35
          sampling_type: slice
        batch:
          max_train_batch_size: 50
        input_mesh_noise: 0.01
        input_point_cloud_noise: 0.0
        euclidian_distance_feature: True
      debug:
        max_tasks_per_split: null
