# This is the main config file for default parameters for all hyperparameter and task parameter.
# Every config file should import this. The project will run with these parameters, no matter what.

name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "gpu"
job-name: "lts_gns"    # this will be the experiments name in slurm

# Required - Cluster Specific
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 2
mem-per-cpu: 15000
time: 1440  # in minutes

sbatch_args: # Dictionary of SBATCH keywords and arguments
  gres: "gpu:1"

slurm_log: "./output/slurmlog" # optional. dir in which slurm output and error logs will be saved.
---
name: "DEFAULT"   # MUST BE DEFAULT

# Implementation default parameters
path: "./output/cw2_data"   # location to save reports in
repetitions: 1 # number of times one set of parameters is run
reps_per_job: 1 # number of repetitions in each job. useful for paralellization. defaults to 1.
reps_in_parallel: 1 # number of repetitions in each job that are executed in parallel. defaults to 1.

iterations: 100  # number of iterations of the algorithm

params:
  exp_version: null
  device: "cuda"

  random_seeds:
    numpy: 69  # can be null for no random seed, "default" for numpy_seed==repetition, or an integer value
    pytorch: 420  # can be null for no random seed, "tied" for pytorch_seed==numpy_seed, or an integer value
    # nice!

  recording:
    config: True  # whether to save the config of each trial as a .yaml file or not
    defaults: True  # whether to log default values (time and memory usage)
    scalars: True   # whether to log scalar values over time, also on hard disk.
    visualizations: True  # whether to log an animation
    checkpoint: True  # whether to log checkpoints of the task. This can e.g, be the networks used for the individual
    # algorithms
    checkpoint_frequency: 100  # checkpointing can quickly become expensive for larger models. We also may only want
    # to do this every n iterations.
    wandb:
      enabled: True  # whether to use the wandb logger or not
      project_name: LTS_GNS  # name of the project
      entity: null  # name of the entity to log to. Will default to your private wandb account
      job_type: null

      # only used categorizing wandb projects
      tags: null  # list of custom tags to sort/find these runs by
      start_method: "thread"  # start method for wandb. "thread" is recommended for slurm and on your cluster.
      # null will use the default wandb start method,
      # which is "fork" on linux and "spawn" on windows (according to copilot)

  algorithm:
    name: lts_gns

    common:
      # parameters that are used by all algorithms
      training:
        batches_per_epoch: 500   # how may minibatches per recorded step. After every epoch, there is the small evaluation of the val tasks.
        add_training_noise: True
      evaluation:
        large:
          recompute_edges: True  # whether to recompute the (mesh-collider) edges for the evaluation or not.
          # if both true and false are in the list, both options are evaluated.
          frequency: 20   # after how many small evals there is a large one
          multi_step_evaluations: [ 1, 5, 10 ]
          animation_task_indices: [ 0, 1]   # which tasks to plot.
          initial_eval: True  # whether to do an initial evaluation after first iteration or not (is slower)
          mse_ggns_style: True  # whether to compute the full rollout mse in the same way as in the GGNS paper or not
      simulator:
        gnn:
          latent_dimension: 128
          share_base: False
          base:
            assert_graph_shapes: False
            scatter_reduce: mean  # how to reduce the edge features for node updates, and the edge and node features for
            # global updates
            # Can be either a single operation or any list of the following: "mean", "sum", "min" or "max", "std".
            create_graph_copy: True  # whether to create a copy of the used graph before the forward pass or not
            stack:
              layer_norm: inner # which kind of layer normalization to use. null/None for no layer norm,
              # "outer" for layer norm around each full message passing step, "inner" for layer norm after each message
              num_steps: 5
              num_step_repeats: 1
              residual_connections: inner
              mlp:
                activation_function: leakyrelu
                num_layers: 1
                add_output_layer: False
                regularization:
                  dropout: 0
                  spectral_norm: False
                  latent_normalization: null
            embedding:
              mlp:
                activation_function: leakyrelu
                num_layers: 1
                add_output_layer: False
                regularization:
                  dropout: 0
                  spectral_norm: False
                  latent_normalization: null
        decoder:
          latent_dimension: 64
        optimizer:
          gnn_learning_rate: 5.0e-4
          decoder_learning_rate: 5.0e-4
        loss: mse  # either "mse" or "chamfer"  # todo move?
        chamfer:
          density_aware: False
          forward_only: False

    cnp:
      simulator:
        likelihood:
          std: 1.0
          graph_aggregation: mean
          timestep_aggregation: sum
      encoder:
        d_r: 8
        gnn:
          latent_dimension: 128
          share_base: False
          base:
            assert_graph_shapes: False
            scatter_reduce: mean  # how to reduce the edge features for node updates, and the edge and node features for
            # global updates
            # Can be either a single operation or any list of the following: "mean", "sum", "min" or "max", "std".
            create_graph_copy: True  # whether to create a copy of the used graph before the forward pass or not
            stack:
              layer_norm: inner # which kind of layer normalization to use. null/None for no layer norm,
              # "outer" for layer norm around each full message passing step, "inner" for layer norm after each message
              num_steps: 5
              residual_connections: inner
              mlp:
                activation_function: leakyrelu
                num_layers: 1
                add_output_layer: False
                regularization:
                  dropout: 0
                  spectral_norm: False
                  latent_normalization: null
            embedding:
              mlp:
                activation_function: leakyrelu
                num_layers: 1
                add_output_layer: False
                regularization:
                  dropout: 0
                  spectral_norm: False
                  latent_normalization: null
    lts_gns_prodmp:
      training:
        num_z_samples_for_elbo_estimate: 32  # how many samples to use for the elbo estimate for the GNN training part
        num_posterior_learner_steps: 1
      evaluation:
        small:
          num_posterior_learner_step: 10
        large:
          num_posterior_learner_steps: 200
          num_z_samples_per_animation_task: 1 # how many different animations with different z samples should be computed
          training_gmm_indices: [ 0, 1 ]  # which gmms to use for the training set

      simulator:

        decoder:
          z_embedding: null  # whether to embed the z into the hidden dimension of the decoder or not.
          feature_dropout: null  # whether to use feature dropout or not. If null, there is no dropout. If a float, this
          # is the dropout rate.
          velocity_decoder: False  # whether to use a velocity decoder or not. If true, the decoder outputs the velocity as well
        prior:
          n_components: 4
          prior_scale: 1.0
          initial_var: 1.0
        likelihood:
          std: 1.0
          chamfer_std: 1.0
          graph_aggregation: mean
          timestep_aggregation: sum
        graph_dropout:
          node_dropout: 0.0
          edge_dropout: 0.0
          global_dropout: 0.0
        mp_config:
          min_tau: 0.3
          max_tau: 3.0
          tau: 1.0
          learn_tau: True
          mp_type: prodmp
          mp_args:
            num_basis: 10
            relative_goal: True
            alpha: 25.0
            alpha_phase: 2.0
            basis_bandwidth_factor: 1.0
            num_basis_outside: 0
            auto_scale_basis: True
            weights_scale: 1.0

      posterior_learner:
        name: multi_daft_posterior_learner
        constant_posterior_learner:
          d_z: 1
        task_properties_learner: # it outputs the task properties as the latent z var
          dummy: null   # no arguments for this learner, but it is required to not have an empty dict
        multi_daft_posterior_learner:
          prior_scale: 1.0    # Standard deviation for distribution which samples the means parameter
          initial_var: 1.0    # Initial Covars is initialized as torch.eye(d_z) * initial_var
          n_components: 3
          d_z: 2
          logging_frequency: 10   # If logging is enabled in the fit() method, the elbo and other metrics are logged every
          # logging_frequency steps. The total logs are bundled into a wandb.Table and plotted
          n_samples_per_comp: 20
          n_samples_per_comp_for_elbo_logging: 100  # elbo between GMM and target density is estimated by sampling from the GMM.
          # This parameter determines how many samples are used for this estimation.
          mini_batch_size_for_target_density: 100  # if n_samples_per_comp (or n_samples_per_comp_for_elbo_logging) * n_components > this, the call for the target_dist
          # log_density() call is split into batches of this size. (For memory reasons)
          sample_at_mean: False
          more:
            component_kl_bound: 0.01
            global_upper_bound: 1.0e+10
            max_prec_element_value: 1.0e+8  # maximum value of precision matrix elements, updates with higher values are rejected
            max_dual_steps: 100
            global_lower_bound: 0.0
            dual_conv_tol: 0.1
            use_warm_starts: False
            warm_start_interval_size: 100

    lts_gns:
      training:
        num_z_samples_for_elbo_estimate: 32  # how many samples to use for the elbo estimate for the GNN training part
        num_posterior_learner_steps: 1
      evaluation:
        small:
          num_posterior_learner_step: 10
        large:
          num_posterior_learner_steps: 200
          num_z_samples_per_animation_task: 1 # how many different animations with different z samples should be computed
          training_gmm_indices: [0, 1]  # which gmms to use for the training set

      simulator:
        use_posterior_target_network: False  # whether to use a target network for the posterior learner or not
        posterior_target_network_rate: 0.99  # 1-rate at which the target network is updated
        decoder:
          z_embedding: null  # whether to embed the z into the hidden dimension of the decoder or not.
          feature_dropout: null  # whether to use feature dropout or not. If null, there is no dropout. If a float, this
                                 # is the dropout rate.
        prior:
          n_components: 4
          prior_scale: 1.0
          initial_var: 1.0
        likelihood:
          std: 1.0
          graph_aggregation: mean
          timestep_aggregation: sum
        graph_dropout:
          node_dropout: 0.0
          edge_dropout: 0.0
          global_dropout: 0.0

      posterior_learner:
        name: multi_daft_posterior_learner
        constant_posterior_learner:
          d_z: 1
        task_properties_learner: # it outputs the task properties as the latent z var
          dummy: null   # no arguments for this learner, but it is required to not have an empty dict
        multi_daft_posterior_learner:
          prior_scale: 1.0    # Standard deviation for distribution which samples the means parameter
          initial_var: 1.0    # Initial Covars is initialized as torch.eye(d_z) * initial_var
          n_components: 3
          d_z: 2
          logging_frequency: 10   # If logging is enabled in the fit() method, the elbo and other metrics are logged every
          # logging_frequency steps. The total logs are bundled into a wandb.Table and plotted
          n_samples_per_comp: 20
          n_samples_per_comp_for_elbo_logging: 100  # elbo between GMM and target density is estimated by sampling from the GMM.
          # This parameter determines how many samples are used for this estimation.
          mini_batch_size_for_target_density: 100  # if n_samples_per_comp (or n_samples_per_comp_for_elbo_logging) * n_components > this, the call for the target_dist
          # log_density() call is split into batches of this size. (For memory reasons)
          sample_at_mean: False
          more:
            component_kl_bound: 0.01
            global_upper_bound: 1.0e+10
            max_prec_element_value: 1.0e+8  # maximum value of precision matrix elements, updates with higher values are rejected
            max_dual_steps: 100
            global_lower_bound: 0.0
            dual_conv_tol: 0.1
            use_warm_starts: False
            warm_start_interval_size: 100
    mgn:
      dummy: null

  env:
    name: deformable_plate
    common:  # common parameters that are used by all environments/tasks
      statistics:
        use_output_normalizer: True  # if true, will use the tasks statistics  to normalize the GNS output for training
      preprocess:
        save_load:
          path_to_datasets: ../datasets/lts_gns/
          load_preprocessed_graph: False  # no load/save shenanigans
          save_preprocessed_graph: False
        task_properties_input_selection: null
        train_label_noise: 0.0   # noise for the labels of the tasks, in the normalized space, so comparable to the likelihood std
        include_pc2mesh: False
        use_collider_velocities: False
        start_index: 0   # start index of the trajectory
      postprocess:
        auxiliary_tasks:
          auxiliary_tasks_per_task: 30
          min_task_length: 1
          max_task_length: 15
          sampling_type: slice
        context_eval_tasks:
          auxiliary_tasks_per_task: 1
          min_task_length: 1
          max_task_length: 10
          sampling_type: slice
        input_mesh_noise: 0.01
        input_point_cloud_noise: 0.0
        euclidian_distance_feature: True
      visualization:
        num_frames: 20  # maximum number of frames to visualize. -1 for "all"
        fps: 4  # how many frames per second, determines the speed of the animation
#        backend: matplotlib  # either matplotlib or plotly. Some tasks only support one or the other
#        limits:
#          xlim: [ -1.25, 1.75 ]
#          ylim: [ -1.25, 1.75 ]
#          zlim: [ -1.25, 1.75 ]
        matplotlib:
          show_legend: False
          fig_size: 10  # fig size in inches
          dpi: 30  # dots per inch
        plotly:
          button_name: "Yoinks"

      debug:
        max_tasks_per_split: null

    multi_modal_toy_task:
      statistics:
        vel_mean: [ -0.0012,  0.0072 ]
        vel_std: [ 0.1649, 0.1756 ]
      preprocess:
        save_load:
          version: direct_start
          file_name: debug_dataset
        use_vel_features: False
      postprocess:
        batch:
          use_adaptive_batch_size: False
          adaptive_batch_size:
            factor_per_context_point: 0.2
            batch_cost: 500
          max_train_batch_size: 50
          max_eval_batch_size: 50
      visualization:
        backend: matplotlib  # either matplotlib or plotly. Some tasks only support one or the other
        limits:
          xlim: [ -1.25, 1.75 ]
          ylim: [ -1.25, 1.75 ]
          zlim: [ -1.25, 1.75 ]

    deformable_plate:
      statistics:
        vel_mean: [-0.0001994771, -0.0058593475]
        vel_std: [0.0034916666, 0.0054799779]
      preprocess:
        save_load:
          file_name: standard_deformable_plate
        use_point_cloud: False
        use_poisson_ratio: True
        use_canonic_mesh_positions: True # whether to include canonic mesh positions ("mesh coordinates")
        # in the node features or not.
        connectivity_setting:
          collider_collider_radius: 0.08
          collider_mesh_radius: 0.3
          world_mesh_radius: null   # null = None in Python. Use if no world edges are used
          # TODO maybe later pointcloud radii
      postprocess:
        batch:
          use_adaptive_batch_size: False
          adaptive_batch_size:
            factor_per_context_point: 0.2
            batch_cost: 200
          max_train_batch_size: 50
          max_eval_batch_size: 50
      visualization:
        backend: matplotlib  # either matplotlib or plotly. Some tasks only support one or the other
        limits:
          xlim: [ -1.25, 1.75 ]
          ylim: [ -1.25, 1.75 ]
          zlim: [ -1.25, 1.75 ]

    tissue_manipulation:
      statistics:
        vel_mean: [ 5.5648877606e-05, -2.6635463655e-05,  9.6673655207e-05 ]
        vel_std: [0.0019695351, 0.0007845265, 0.0016459003]
      preprocess:
        save_load:
          file_name: standard_tissue_manipulation  # only relevant if save_load.{save, load}_preprocessed_graph is True
        use_point_cloud: False
        use_poisson_ratio: True
        use_canonic_mesh_positions: True # whether to include canonic mesh positions ("mesh coordinates")
        # in the node features or not.
        connectivity_setting:
          # todo check
          collider_collider_radius: 0.08
          collider_mesh_radius: 0.2
          world_mesh_radius: null   # Use if no world edges are used
      postprocess:
        batch:
          use_adaptive_batch_size: False
          adaptive_batch_size:
            factor_per_context_point: 0.2
            batch_cost: 80
          max_train_batch_size: 50
          max_eval_batch_size: 50
      visualization:
        backend: plotly  # either matplotlib or plotly. Some tasks only support one or the other
        limits:
          xlim: [ -1.0, 1.0 ]
          ylim: [ -1.0, 1.0 ]
          zlim: [ -1.5, 2.0 ]

    cavity_grasping:
      debug:
        max_rollout_length: null
      statistics:
        vel_mean: [ 0.0, 0.0, 0.0 ]
        vel_std: [1.0, 1.0, 1.0]
      preprocess:
        save_load:
          file_name: standard_cavity_grasping  # only relevant if save_load.{save, load}_preprocessed_graph is True
        use_point_cloud: False
        use_poisson_ratio: True
        use_canonic_mesh_positions: True # whether to include canonic mesh positions ("mesh coordinates")
        # in the node features or not.
        connectivity_setting:
          # todo check
          collider_collider_radius: 0.08
          collider_mesh_radius: 0.2
          world_mesh_radius: null   # Use if no world edges are used
        start_index: 11   # remove initial non-linear movement of the gripper
      postprocess:
        batch:
          use_adaptive_batch_size: False
          adaptive_batch_size:
            factor_per_context_point: 0.2
            batch_cost: 80
          max_train_batch_size: 1
          max_eval_batch_size: 1
      visualization:
        backend: plotly  # either matplotlib or plotly. Some tasks only support one or the other
        limits:
          xlim: [ -1.0, 1.0 ]
          ylim: [ -1.0, 1.0 ]
          zlim: [ -1.5, 2.0 ]
    multi_decision_toy_task:
      statistics:
        vel_mean: [0.0095988382, 0.0002336661]
        vel_std: [0.0219038352, 0.0169322267]
      preprocess:
        use_point_cloud: False
        save_load:
          file_name: debug_dataset
          version: null
        use_pos_features: True
        use_time_features: True
      postprocess:
        batch:
          use_adaptive_batch_size: False
          adaptive_batch_size:
            factor_per_context_point: 0.2
            batch_cost: 500
          max_train_batch_size: 50
          max_eval_batch_size: 50
      visualization:
        backend: matplotlib  # either matplotlib or plotly. Some tasks only support one or the other
        limits:
          xlim: [ -1.25, 1.75 ]
          ylim: [ -1.25, 1.75 ]
          zlim: [ -1.25, 1.75 ]

    pybullet_uniform:
      statistics:
        vel_mean: [0.0001, 0.0009, 0.0001]
        vel_std: [0.0005, 0.0011, 0.0003]
      preprocess:
        save_load:
          version: "1"
        connectivity_setting:
          collider_mesh_radius: 0.3
          world_mesh_radius: null   # null = None in Python. Use if no world edges are used
        use_canonic_mesh_positions: True
      postprocess:
        batch:
          use_adaptive_batch_size: False
          adaptive_batch_size:
            factor_per_context_point: 0.2
            batch_cost: 100
          max_train_batch_size: 50
          max_eval_batch_size: 50
      visualization:
        backend: plotly  # either matplotlib or plotly. Some tasks only support one or the other
        limits:
          xlim: [ 0.0, 1.0 ]
          ylim: [ 0.0, 1.0 ]
          zlim: [ 0.0, 1.0 ]

    pybullet_square_cloth:
      statistics:
        vel_mean: [-1.5800746041e-05,  8.2708970876e-04,  1.6668645549e-04]
        vel_std: [0.0001689098, 0.0010139241, 0.0002227605]
      preprocess:
        save_load:
          version: null
        connectivity_setting:
          collider_mesh_radius: 0.075
          world_mesh_radius: null   # null = None in Python. Use if no world edges are used
        use_canonic_mesh_positions: True
      postprocess:
        batch:
          use_adaptive_batch_size: False
          adaptive_batch_size:
            factor_per_context_point: 0.2
            batch_cost: 100
          max_train_batch_size: 50
          max_eval_batch_size: 50
      visualization:
        backend: plotly  # either matplotlib or plotly. Some tasks only support one or the other
        limits:
          xlim: [ 0.0, 1.0 ]
          ylim: [ 0.0, 1.0 ]
          zlim: [ 0.0, 1.0 ]
